{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasaman-habibi/Analysis-of-Sustainability-Reports/blob/main/FinalStep_02_FilteredData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uEjtTaENL7Ed"
      },
      "outputs": [],
      "source": [
        "from google.colab import files, drive\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW0CG1o0AzPa",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. آپلود فایل‌های متنی\n",
        "uploaded_Texts = files.upload()\n",
        "\n",
        "Combined_path = \"/content/drive/MyDrive/Combined_Texts\"\n",
        "os.makedirs(Combined_path, exist_ok=True)\n",
        "Combined_file = os.path.join(Combined_path, \"Combined_Texts.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PLDcVFtAL1Yv"
      },
      "outputs": [],
      "source": [
        "# 2. آپلود فایل دیکشنری Loughran-McDonald\n",
        "\n",
        "uploaded_Dic = files.upload()\n",
        "for filename in uploaded_Dic.keys():\n",
        "    df_dict = pd.read_excel(io.BytesIO(uploaded_Dic[filename]))\n",
        "\n",
        "Dic_path = \"/content/drive/MyDrive/sustainability_table\"\n",
        "os.makedirs(Dic_path, exist_ok=True)\n",
        "Dic_file = os.path.join(Dic_path, \"Loughran-McDonald.xlsx\")\n",
        "\n",
        "df_dict.to_excel(Dic_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p5mmxr9TnSe3"
      },
      "outputs": [],
      "source": [
        "#برای خوندن فایل اکسل تا نیاز به هر بار اپلود نباشد\n",
        "\n",
        "df_dict = pd.read_excel(\"/content/drive/MyDrive/sustainability_table/Loughran-McDonald.xlsx\")\n",
        "keywords = pd.read_excel(\"/content/drive/MyDrive/sustainability_table/keywords.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rGP0fFpNszBg"
      },
      "outputs": [],
      "source": [
        "# 3. ساخت فایل keywords\n",
        "\n",
        "keywords_path = \"/content/drive/MyDrive/sustainability_table\"\n",
        "os.makedirs(keywords_path, exist_ok=True)\n",
        "keywords_file = os.path.join(keywords_path, \"keywords.xlsx\")\n",
        "\n",
        "# لیست کلمات کلیدی نهایی\n",
        "default_keywords = [\n",
        "     \"Sustainable\" , \"Sustainability\" ,\n",
        "     \"Development\" , \"Warming\" , \"Resource\" ,\n",
        "     \"Environmental\" ,  \"governance\" , \"environment\",\n",
        "     \"climate\" , \"Recycling\" , \"Energy\",\n",
        "     \"Economic\" , \"Economy\" , \"Green\",\n",
        "     \"Social\" , \"Society\" , \"materials\" , \"Ecosystem\"\n",
        "     \"Carbon\", \"footprint\" , \"Pollutants \" ,\n",
        "     \"Natural\" , \"Earth\", \"Air\", \"biodiversity\",  \"Crisis\" ,\n",
        "      \"Atmospheric\" , \"Water\" , \"pollution\" ,\n",
        "     \"Drought\" , \"Famine\" , \"Decarbonization\" ,\n",
        "     \"Species\" , \"Demographic\" , \"Global\" ,\n",
        "      \"Waste\" , \"agriculture\" , \"innovation\",\n",
        "     ]\n",
        "\n",
        "updated_keywords_df = pd.DataFrame(sorted(set(default_keywords)), columns=[\"keyword\"])\n",
        "updated_keywords_df.to_excel(keywords_file, index=False)\n",
        "keywords = updated_keywords_df[\"keyword\"].str.lower().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2AYfT8VMgq4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 4. ساخت مجموعه واژه‌های پایداری از دیکشنری بر مبنای کلیدواژه نهایی\n",
        "\n",
        "keywords_set = set(keywords)\n",
        "\n",
        "sustain_terms = set()\n",
        "for word in df_dict[\"Word\"]:\n",
        "    word_lower = str(word).strip().lower()\n",
        "    if word_lower in keywords_set:\n",
        "        sustain_terms.add(word_lower)\n",
        "\n",
        "print(f\"{len(sustain_terms)} sustainability-related terms extracted.\")\n",
        "\n",
        "sustain_Dic = df_dict[df_dict[\"Word\"].str.lower().isin(sustain_terms)]\n",
        "\n",
        "sustain_Dic_path = \"/content/drive/MyDrive/sustainability_table\"\n",
        "os.makedirs(sustain_Dic_path, exist_ok=True)\n",
        "sustain_Dic_file = os.path.join(sustain_Dic_path, \"sustain_Dic.xlsx\")\n",
        "\n",
        "sustain_Dic.to_excel(sustain_Dic_file, index=False)\n",
        "\n",
        "print(f\"Filtered Excel file saved to: {sustain_Dic_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-IkEJDJCMoMH"
      },
      "outputs": [],
      "source": [
        "# 5. خواندن فایل های متنی برای انجام process\n",
        "\n",
        "text = \"\"\n",
        "for filename in uploaded_Texts.keys():\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "        text += file.read() + \"\\n\\n\"\n",
        "\n",
        "def split_paragraphs(text):\n",
        "    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n",
        "    return paragraphs\n",
        "\n",
        "paragraphs = split_paragraphs(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpIDYCpcc6wj"
      },
      "outputs": [],
      "source": [
        "# 6. فیلتر کردن متن با استفاده از دیکشنری ساخته شده بر اساس کلمات پایداری\n",
        "\n",
        "sustain_Dic_path = \"/content/drive/MyDrive/sustainability_table/sustain_Dic.xlsx\"\n",
        "sustain_Dic = pd.read_excel(sustain_Dic_path)\n",
        "\n",
        "sustain_terms = set(sustain_Dic[\"Word\"].dropna().str.lower())\n",
        "\n",
        "\n",
        "#  تابع بررسی وجود حداقل یک واژه پایداری در هر پاراگراف\n",
        "# def contains_sustain_terms(paragraph):\n",
        "#     words = paragraph.lower().split()\n",
        "#     words_clean = [word.strip(\".,;:!?()[]{}\\\"'\") for word in words]\n",
        "#     return any(word in sustain_terms for word in words_clean)\n",
        "\n",
        "#فرایند برای یک کلمه یا چند کلمه بعنوان یک عبارت در لیست کلیدواژه\n",
        "sustain_terms_raw = sustain_Dic[\"Word\"].dropna().str.strip().str.lower().tolist()\n",
        "single_terms = set(term for term in sustain_terms_raw if len(term.split()) == 1)\n",
        "multi_terms = set(term for term in sustain_terms_raw if len(term.split()) > 1)\n",
        "\n",
        "def contains_sustain_terms(paragraph):\n",
        "    para_lower = paragraph.lower()\n",
        "\n",
        "    #  عبارات چندکلمه‌ای\n",
        "    padded_para = f\" {para_lower} \"\n",
        "    match_multi = any(f\" {term} \" in padded_para for term in multi_terms)\n",
        "\n",
        "    #  کلمات تکی\n",
        "    words_in_para = re.findall(r'\\b\\w+\\b', para_lower)\n",
        "    match_single = any(word in single_terms for word in words_in_para)\n",
        "\n",
        "    return match_multi or match_single\n",
        "\n",
        "# فیلتر کردن پاراگراف‌های مرتبط با پایداری\n",
        "filtered_paragraphs = [para for para in paragraphs if contains_sustain_terms(para)]\n",
        "\n",
        "print(f\"{len(filtered_paragraphs)} paragraphs contain sustainability-related terms.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HK5kC-kjzkm"
      },
      "outputs": [],
      "source": [
        "# 7. ترکیب فقط پاراگراف‌های مرتبط با پایداری\n",
        "\n",
        "Combined_Sustain_text_path = os.path.join(Combined_path, \"Combined_Sustain_text.txt\")\n",
        "\n",
        "# بررسی شماره فایل خروجی\n",
        "def get_next_index(base_path, prefix=\"Combined_Sustain_text_\", suffix=\".txt\"):\n",
        "    existing_files = os.listdir(base_path)\n",
        "    indices = []\n",
        "\n",
        "    for fname in existing_files:\n",
        "        match = re.match(fr\"{re.escape(prefix)}(\\d+){re.escape(suffix)}\", fname)\n",
        "        if match:\n",
        "            indices.append(int(match.group(1)))\n",
        "    return max(indices, default=0) + 1\n",
        "\n",
        "\n",
        "file_index = get_next_index(Combined_path)\n",
        "output_filename = f\"Combined_Sustain_text_{file_index}.txt\"\n",
        "output_path = os.path.join(Combined_path, output_filename)\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for filename in uploaded_Texts.keys():\n",
        "        with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as infile:\n",
        "            content = infile.read()\n",
        "            paragraphs = split_paragraphs(content)\n",
        "            filtered_paragraphs = [para for para in paragraphs if contains_sustain_terms(para)]\n",
        "\n",
        "            if filtered_paragraphs:\n",
        "                outfile.write(f\"===== Start of File: {filename} =====\\n\")\n",
        "                for para in filtered_paragraphs:\n",
        "                    outfile.write(para + \"\\n\\n\")\n",
        "                outfile.write(f\"===== End of File: {filename} =====\\n\\n\")\n",
        "\n",
        "print(f\" پاراگراف‌های فیلتر شده ذخیره شدند در: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEa3bHv4oyiI"
      },
      "outputs": [],
      "source": [
        "#8. ترکیب فایل های شماره گذاری ساخته شده\n",
        "\n",
        "import glob\n",
        "\n",
        "merged_output_path = os.path.join(Combined_path, \"Combined_Sustain_MERGED.txt\")\n",
        "\n",
        "files_to_merge = sorted(\n",
        "    glob.glob(os.path.join(Combined_path, \"Combined_Sustain_text_*.txt\")),\n",
        "    key=lambda x: int(re.search(r\"_(\\d+)\\.txt$\", x).group(1))\n",
        ")\n",
        "\n",
        "with open(merged_output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for file_path in files_to_merge:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
        "            content = infile.read()\n",
        "            outfile.write(content + \"\\n\")\n",
        "\n",
        "print(f\" همه فایل‌ها با موفقیت ترکیب شدند.\\n مسیر فایل نهایی: {merged_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. تعداد تکرار کلیدواژه ها در هر فایل\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "file_keyword_counts = {}\n",
        "\n",
        "sustain_terms_list = list(sustain_terms)\n",
        "\n",
        "for filename in uploaded_Texts.keys():\n",
        "    with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        content = f.read().lower()\n",
        "\n",
        "    counts = {}\n",
        "    for term in sustain_terms_list:\n",
        "        pattern = r'\\b' + re.escape(term) + r'\\b'\n",
        "        matches = re.findall(pattern, content)\n",
        "        if matches:\n",
        "            counts[term] = len(matches)\n",
        "\n",
        "    file_keyword_counts[filename] = counts\n",
        "\n",
        "for fname, count_dict in file_keyword_counts.items():\n",
        "    print(f\"\\n File: {fname}\")\n",
        "    for kw, cnt in sorted(count_dict.items(), key=lambda x: -x[1]):\n",
        "        print(f\"   {kw} : {cnt}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TRljBB4pLOJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#گزارش تجمیعی از تعداد تکرار کلیدواژه ها در فایل های متنی\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "report_path = \"/content/drive/MyDrive/sustainability_table/final_keyword_report.xlsx\"\n",
        "\n",
        "records = []\n",
        "for fname, count_dict in file_keyword_counts.items():\n",
        "    for kw, cnt in count_dict.items():\n",
        "        records.append({\"filename\": fname, \"keyword\": kw, \"count\": cnt})\n",
        "\n",
        "new_df = pd.DataFrame(records)\n",
        "\n",
        "#ترکیب گزارشات در هر بار اپلود فایل\n",
        "if os.path.exists(report_path):\n",
        "    old_df = pd.read_excel(report_path)\n",
        "    combined_df = pd.concat([old_df, new_df], ignore_index=True)\n",
        "else:\n",
        "    combined_df = new_df\n",
        "\n",
        "pivot_df = combined_df.pivot_table(\n",
        "    index=\"filename\",\n",
        "    columns=\"keyword\",\n",
        "    values=\"count\",\n",
        "    aggfunc=\"sum\",\n",
        "    fill_value=0\n",
        ")\n",
        "\n",
        "pivot_df = pivot_df[sorted(pivot_df.columns)]\n",
        "\n",
        "pivot_df.to_excel(report_path)\n",
        "\n",
        "print(f\" گزارش Pivot ذخیره شد در:\\n{report_path}\")"
      ],
      "metadata": {
        "id": "yjxxZuLGZWDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAksBB9w5E/dUEYG0nKl/m",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}