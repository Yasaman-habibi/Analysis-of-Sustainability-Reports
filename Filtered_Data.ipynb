{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasaman-habibi/Analysis-of-Sustainability-Reports/blob/main/Filtered_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEjtTaENL7Ed"
      },
      "outputs": [],
      "source": [
        "from google.colab import files, drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import io\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW0CG1o0AzPa",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. آپلود فایل‌های متنی\n",
        "uploaded_Texts = files.upload()\n",
        "\n",
        "Combined_path = \"/content/drive/MyDrive/Combined_Texts\"\n",
        "os.makedirs(Combined_path, exist_ok=True)\n",
        "Combined_file = os.path.join(Combined_path, \"Combined_Texts.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PLDcVFtAL1Yv"
      },
      "outputs": [],
      "source": [
        "# 2. آپلود فایل دیکشنری Loughran-McDonald\n",
        "\n",
        "uploaded_Dic = files.upload()\n",
        "for filename in uploaded_Dic.keys():\n",
        "    df_dict = pd.read_excel(io.BytesIO(uploaded_Dic[filename]))\n",
        "\n",
        "Dic_path = \"/content/drive/MyDrive/sustainability_table\"\n",
        "os.makedirs(Dic_path, exist_ok=True)\n",
        "Dic_file = os.path.join(Dic_path, \"Loughran-McDonald.xlsx\")\n",
        "\n",
        "df_dict.to_excel(Dic_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5mmxr9TnSe3"
      },
      "outputs": [],
      "source": [
        "df_dict = pd.read_excel(\"/content/drive/MyDrive/sustainability_table/Loughran-McDonald.xlsx\")\n",
        "keywords = pd.read_excel(\"/content/drive/MyDrive/sustainability_table/keywords.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGP0fFpNszBg"
      },
      "outputs": [],
      "source": [
        "# 3. ساخت فایل keywords\n",
        "keywords_path = \"/content/drive/MyDrive/sustainability_table\"\n",
        "os.makedirs(keywords_path, exist_ok=True)\n",
        "keywords_file = os.path.join(keywords_path, \"keywords.xlsx\")\n",
        "\n",
        "# لیست کلمات کلیدی نهایی\n",
        "default_keywords = [\n",
        "     \"Sustainable\" , \"Sustainability\" , \"Sustainable finance\" , \"Sustainable innovation\" , \"Sustainable agriculture\" ,\n",
        "     \"Sustainable materials\" ,  \"Sustainable supply chain\" , \"Sustainable development\" ,\n",
        "     \"SDGs\" , \"ESG\", \"Non-Financial\" , \"Development\" ,\n",
        "     \"Environmental\" ,  \"Environmental, social, and governance\" , \"Environmental protection\", \"Environmental impact\" , \"environment\",\n",
        "     \"climate\" , \"Climate mitigation\" , \"Climate change\" ,\n",
        "     \"Economic\" , \"Economic sustainability\" , \"Economy\" , \"Circular economy\" , \"Green economy\" , \"Green technology\" ,\n",
        "     \"Social\" , \"Society\" , \"Corporate social responsibility\" ,\"CSR\" ,\n",
        "     \"Carbon footprint\", \"Carbon emissions\" ,  \"Pollutants \" ,  \"Greenhouse Gas Emissions\" ,  \"Decarbonization\" ,\n",
        "     \"Renewable energy\" , \"Clean energy\" , \"Energy efficiency\" , \"Recycling\" , \"Demographic changes\" ,\n",
        "     \"Waste management\" , \"Zero waste\" , \"Natural Resources\" , \"Resource management\" ,\n",
        "     \"Earth\", \"Air\", \"biodiversity\",  \"Crisis\" ,  \"Atmospheric\" , \"Water\" , \"pollution\" , \"Pollution reduction\" ,\n",
        "     \"Drought\" , \"Famine\" ,  \"Water conservation\" ,\n",
        "     \"Ground Warming\" , \"Global Warming\" , \"Species extinction\" , \"Ecosystem preservation\" ,\n",
        "     \"Future Needs\" ,  \"Life cycle assessment\" , \"Eco-friendly\" , \"Responsible consumption\" , \"Human rights\" ,\n",
        "     \"Better life\",\n",
        "]\n",
        "\n",
        "updated_keywords_df = pd.DataFrame(sorted(set(default_keywords)), columns=[\"keyword\"])\n",
        "updated_keywords_df.to_excel(keywords_file, index=False)\n",
        "keywords = updated_keywords_df[\"keyword\"].str.lower().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2AYfT8VMgq4"
      },
      "outputs": [],
      "source": [
        "# 4. ساخت مجموعه واژه‌های پایداری از دیکشنری بر مبنای keywords\n",
        "\n",
        "sustain_terms = set()\n",
        "for word in df_dict[\"Word\"]:\n",
        "    word_lower = str(word).lower()\n",
        "    if any(key in word_lower for key in keywords):\n",
        "        sustain_terms.add(word_lower)\n",
        "\n",
        "print(f\"{len(sustain_terms)} sustainability-related terms extracted.\")\n",
        "\n",
        "\n",
        "#  ذخیره ردیف‌های دیکشنری دز یک فایل جدید با استفاده از sustain_terms\n",
        "\n",
        "sustain_Dic = df_dict[df_dict[\"Word\"].str.lower().isin(sustain_terms)]\n",
        "\n",
        "\n",
        "sustain_Dic_path = \"/content/drive/MyDrive/sustainability_table\"\n",
        "os.makedirs(sustain_Dic_path, exist_ok=True)\n",
        "sustain_Dic_file = os.path.join(sustain_Dic_path, \"sustain_Dic.xlsx\")\n",
        "sustain_Dic.to_excel(sustain_Dic_file, index=False)\n",
        "\n",
        "print(f\"Filtered Excel file saved to: {sustain_Dic_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IkEJDJCMoMH"
      },
      "outputs": [],
      "source": [
        "# 5. خواندن فایل های متنی برای انجام process\n",
        "text = \"\"\n",
        "for filename in uploaded_Texts.keys():\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "        text += file.read() + \"\\n\\n\"\n",
        "\n",
        "def split_paragraphs(text):\n",
        "    paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n",
        "    return paragraphs\n",
        "\n",
        "paragraphs = split_paragraphs(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpIDYCpcc6wj"
      },
      "outputs": [],
      "source": [
        "# 6: فیلتر کردن متن با استفاده از دیکشنری ساخته شده بر اساس کلمات پایداری\n",
        "\n",
        "sustain_Dic_path = \"/content/drive/MyDrive/sustainability_table/sustain_Dic.xlsx\"\n",
        "sustain_Dic = pd.read_excel(sustain_Dic_path)\n",
        "\n",
        "\n",
        "sustain_terms = set(sustain_Dic[\"Word\"].dropna().str.lower())\n",
        "\n",
        "\n",
        "def contains_sustain_terms(paragraph):\n",
        "    words = paragraph.lower().split()\n",
        "    words_clean = [word.strip(\".,;:!?()[]{}\\\"'\") for word in words]\n",
        "    return any(word in sustain_terms for word in words_clean)\n",
        "\n",
        "\n",
        "# فیلتر کردن پاراگراف‌های مرتبط با پایداری\n",
        "filtered_paragraphs = [para for para in paragraphs if contains_sustain_terms(para)]\n",
        "\n",
        "print(f\"{len(filtered_paragraphs)} paragraphs contain sustainability-related terms.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HK5kC-kjzkm"
      },
      "outputs": [],
      "source": [
        "# 7. ترکیب فقط پاراگراف‌های مرتبط با پایداری\n",
        "\n",
        "Combined_Sustain_text_path = os.path.join(Combined_path, \"Combined_Sustain_text.txt\")\n",
        "\n",
        "\n",
        "def get_next_index(base_path, prefix=\"Combined_Sustain_text_\", suffix=\".txt\"):\n",
        "    existing_files = os.listdir(base_path)\n",
        "    indices = []\n",
        "\n",
        "    for fname in existing_files:\n",
        "        match = re.match(fr\"{re.escape(prefix)}(\\d+){re.escape(suffix)}\", fname)\n",
        "        if match:\n",
        "            indices.append(int(match.group(1)))\n",
        "    return max(indices, default=0) + 1\n",
        "\n",
        "\n",
        "file_index = get_next_index(Combined_path)\n",
        "output_filename = f\"Combined_Sustain_text_{file_index}.txt\"\n",
        "output_path = os.path.join(Combined_path, output_filename)\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for filename in uploaded_Texts.keys():\n",
        "        with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as infile:\n",
        "            content = infile.read()\n",
        "\n",
        "            paragraphs = split_paragraphs(content)\n",
        "\n",
        "            filtered_paragraphs = [para for para in paragraphs if contains_sustain_terms(para)]\n",
        "\n",
        "\n",
        "            if filtered_paragraphs:\n",
        "                outfile.write(f\"===== Start of File: {filename} =====\\n\")\n",
        "                for para in filtered_paragraphs:\n",
        "                    outfile.write(para + \"\\n\\n\")\n",
        "                outfile.write(f\"===== End of File: {filename} =====\\n\\n\")\n",
        "\n",
        "print(f\" پاراگراف‌های فیلتر شده ذخیره شدند در: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IcEgZhotZhNb"
      },
      "outputs": [],
      "source": [
        "uploaded_Merged = files.upload()\n",
        "\n",
        "\n",
        "merged_output_path = os.path.join(Combined_path, \"Combined_Sustain_MERGED.txt\")\n",
        "\n",
        "with open(Combined_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for filename in uploaded_Merged.keys():\n",
        "        with open(filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as infile:\n",
        "            content = infile.read()\n",
        "            outfile.write(content + \"\\n\")\n",
        "\n",
        "print(f\"All text files have been combined into {Combined_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEa3bHv4oyiI"
      },
      "outputs": [],
      "source": [
        "#8. ترکیب فایل های شماره گذاری ساخته شده\n",
        "\n",
        "import glob\n",
        "\n",
        "merged_output_path = os.path.join(Combined_path, \"Combined_Sustain_MERGED.txt\")\n",
        "\n",
        "\n",
        "files_to_merge = sorted(\n",
        "    glob.glob(os.path.join(Combined_path, \"Combined_Sustain_text_*.txt\")),\n",
        "    key=lambda x: int(re.search(r\"_(\\d+)\\.txt$\", x).group(1))\n",
        ")\n",
        "\n",
        "\n",
        "with open(merged_output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for file_path in files_to_merge:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
        "            content = infile.read()\n",
        "            outfile.write(content + \"\\n\")\n",
        "\n",
        "\n",
        "print(f\" همه فایل‌ها با موفقیت ترکیب شدند.\\n مسیر فایل نهایی: {merged_output_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYGB906b2Y4bxVeOS3dzjd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}