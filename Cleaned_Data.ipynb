{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoVJyVUfOu9xFslFLFZ8Zg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasaman-habibi/Pre_Processing_Report/blob/main/Cleaned_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c8vmGDEFWgc5"
      },
      "outputs": [],
      "source": [
        "#instal Library\n",
        "!pip install nltk\n",
        "!pip install textblob\n",
        "!pip install emoji\n",
        "!pip install clean-text[gpl]\n",
        "!pip install symspellpy\n",
        "\n",
        "!pip install symspellpy spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Library\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import nltk\n",
        "import emoji\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from nltk import pos_tag\n",
        "from cleantext import clean\n",
        "import ipywidgets as widgets\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from IPython.display import display\n",
        "from nltk.tokenize import word_tokenize\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from ipywidgets import SelectMultiple, Button, VBox, Layout"
      ],
      "metadata": {
        "id": "BHb_i8X2WrGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v6K5XGSvWs4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload Files\n",
        "from google.colab import drive, files\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "source_path = \"/content/drive/MyDrive/Combined_Texts\"\n",
        "all_txt_files = glob.glob(os.path.join(source_path, \"*.txt\"))\n",
        "selector = widgets.SelectMultiple(\n",
        "    options=all_txt_files,\n",
        "    description='Select files',\n",
        "    rows=10\n",
        ")\n",
        "display(selector)"
      ],
      "metadata": {
        "id": "3AVDFRYoWw8j",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_Texts = list(selector.value)\n",
        "Cleaned_path = \"/content/drive/MyDrive/Cleaned_Texts\"\n",
        "os.makedirs(Cleaned_path, exist_ok=True)\n",
        "\n",
        "print(\" انتخاب شد:\\n\" + \"\\n\".join(uploaded_Texts))"
      ],
      "metadata": {
        "id": "I41cY7tbW0M7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Config\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = \"/content/drive/MyDrive/sustainability_table/frequency_dictionary_en_82_765.txt\"\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
        "nlp.max_length = 50000000\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "nltk_stopwords = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "7ENTVc7IW5Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess_text Function\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text, do_spell_check=True, remove_stopwords=True, use_stem=True, chunk_size=50000):\n",
        "    if not text or text.strip() == \"\":\n",
        "        return []\n",
        "\n",
        "    def process_chunk(chunk):\n",
        "        chunk = re.sub(r'(.)\\1{2,}', r'\\1', chunk)\n",
        "\n",
        "        #spell Check\n",
        "        if do_spell_check:\n",
        "            corrected_words = []\n",
        "            for word in chunk.split():\n",
        "                if re.search(r'[A-Z0-9]', word):\n",
        "                    corrected_words.append(word)\n",
        "                else:\n",
        "                    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
        "                    corrected_words.append(suggestions[0].term if suggestions else word)\n",
        "            chunk = ' '.join(corrected_words)\n",
        "\n",
        "        #Normalize\n",
        "        chunk = re.sub(r'[^\\w\\s]', '', chunk)\n",
        "        chunk = re.sub(r'\\s+', ' ', chunk).strip()\n",
        "        chunk = re.sub(r'\\d+', '', chunk)\n",
        "\n",
        "        chunk = chunk.lower()\n",
        "\n",
        "        doc = nlp(chunk)\n",
        "\n",
        "        #Remove Stop Words\n",
        "        #lemmatization and stemming\n",
        "        #Tokenize\n",
        "        if use_stem:\n",
        "            return [(stemmer.stem(token.lemma_), token.tag_)\n",
        "                    for token in doc\n",
        "                    if not remove_stopwords or not token.is_stop]\n",
        "        else:\n",
        "            return [(token.lemma_, token.tag_)\n",
        "                    for token in doc\n",
        "                    if not remove_stopwords or not token.is_stop]\n",
        "\n",
        "    # Divide text into chunks\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    # Merge Outputs\n",
        "    tokens_out = []\n",
        "    for chunk in chunks:\n",
        "        tokens_out.extend(process_chunk(chunk))\n",
        "\n",
        "    return tokens_out"
      ],
      "metadata": {
        "id": "fLlrI7amsCp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch Processing\n",
        "\n",
        "from time import sleep\n",
        "\n",
        "filenames = uploaded_Texts\n",
        "batch_size = 5\n",
        "cleaned_paragraphs = []\n",
        "\n",
        "\n",
        "def process_file(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "\n",
        "    cleaned = preprocess_text(content, do_spell_check=True, remove_stopwords=True, use_stem=True)\n",
        "\n",
        "    cleaned_text_str = ' '.join([f\"{word}/{pos}\" for word, pos in cleaned])\n",
        "\n",
        "    base_name = os.path.splitext(os.path.basename(filepath))[0]\n",
        "    output_filename = f\"cleaned_{base_name}.txt\"\n",
        "    output_path = os.path.join(Cleaned_path, output_filename)\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
        "        out_file.write(cleaned_text_str)\n",
        "\n",
        "    print(f\"فایل تمیزشده ذخیره شد: {output_filename}\")\n",
        "    return cleaned\n",
        "\n",
        "#ThreadPoolExecutor\n",
        "for i in range(0, len(filenames), batch_size):\n",
        "    batch_files = filenames[i:i+batch_size]\n",
        "    print(f\"در حال پردازش فایل‌های {i+1} تا {i+len(batch_files)} از {len(filenames)}...\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        results = list(executor.map(process_file, batch_files))\n",
        "\n",
        "    cleaned_paragraphs.extend(results)\n",
        "\n",
        "    sleep(0.5)\n",
        "\n",
        "print(f\"\\nتمام فایل‌ها پردازش شدند و متن‌ها در حافظه نگهداری شدند.\")\n"
      ],
      "metadata": {
        "id": "W_L4J1yusHfI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}