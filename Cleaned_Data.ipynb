{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasaman-habibi/Analysis-of-Sustainability-Reports/blob/main/Cleaned_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kSERZUBQPQWK"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install textblob\n",
        "!pip install emoji\n",
        "!pip install clean-text[gpl]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMpMRZFZWZHc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from cleantext import clean\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive, files\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RS5eqKHaWZEd"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TYgkkHirarFX"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "uploaded_Texts = files.upload()\n",
        "\n",
        "Cleaned_path = \"/content/drive/MyDrive/Cleaned_Texts\"\n",
        "os.makedirs(Cleaned_path, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM2Orh-gWZBl"
      },
      "outputs": [],
      "source": [
        "# تعریف تابع پیش‌پردازش\n",
        "def preprocess_text(text, do_spell_check=True):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = clean(text,\n",
        "                 no_urls=True,\n",
        "                 no_emails=True,\n",
        "                 no_emoji=True,\n",
        "                 no_punct=False)\n",
        "\n",
        "    if do_spell_check:\n",
        "        text = str(TextBlob(text).correct())            # اصلاح غلط‌های املایی\n",
        "\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)            # حذف تکرار حروف\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)                 # حذف علائم نگارشی\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()            # حذف فاصله‌های اضافی\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))        # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()                           # Lemmatize + stem\n",
        "    tokens = [stemmer.stem(lemmatizer.lemmatize(token)) for token in tokens]\n",
        "\n",
        "    pos_tags = pos_tag(tokens)                          # POS tagging\n",
        "\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HXyy95p3wZOh"
      },
      "outputs": [],
      "source": [
        "# تنظیمات batch\n",
        "#ذخیره هر فایل بصورت جداگانه\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('all', halt_on_error=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4zygz8vRegix"
      },
      "outputs": [],
      "source": [
        "filenames = list(uploaded_Texts.keys())\n",
        "batch_size = 5  # تعداد فایل در هر دسته\n",
        "\n",
        "for i in range(0, len(filenames), batch_size):\n",
        "    batch_files = filenames[i:i+batch_size]\n",
        "    print(f\" در حال پردازش دسته {i+1} تا {i+len(batch_files)} از {len(filenames)} فایل...\")\n",
        "\n",
        "    for filename in batch_files:\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "            content = file.read()\n",
        "            cleaned_text = preprocess_text(content)\n",
        "\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        output_filename = f\"cleaned_{base_name}.txt\"\n",
        "        output_path = os.path.join(Cleaned_path, output_filename)\n",
        "\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
        "            out_file.write(cleaned_text)\n",
        "\n",
        "        print(f\" فایل تمیز‌شده ذخیره شد: {output_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WB3hKTwBP7l7"
      },
      "outputs": [],
      "source": [
        "#یا ذخیره بصورت ترکیبی در یک فایل\n",
        "# تنظیمات batch\n",
        "filenames = list(uploaded_Texts.keys())\n",
        "batch_size = 5\n",
        "\n",
        "cleaned_paragraphs = []\n",
        "\n",
        "for i in range(0, len(filenames), batch_size):\n",
        "    batch_files = filenames[i:i+batch_size]\n",
        "    print(f\" در حال پردازش فایل‌های {i+1} تا {i+len(batch_files)} از {len(filenames)}...\")\n",
        "\n",
        "    for filename in batch_files:\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "            content = file.read()\n",
        "            cleaned = preprocess_text(content)\n",
        "            cleaned_paragraphs.append(cleaned)\n",
        "\n",
        "    sleep(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuavGr41P7Wp"
      },
      "outputs": [],
      "source": [
        "with open(Cleaned_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for para in cleaned_paragraphs:\n",
        "        f.write(para + \"\\n\\n\")\n",
        "\n",
        "print(f\"\\n تمام فایل‌ها پردازش و ذخیره شدند: {Cleaned_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVsmNscQI+CdK0Wqw7oeV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}